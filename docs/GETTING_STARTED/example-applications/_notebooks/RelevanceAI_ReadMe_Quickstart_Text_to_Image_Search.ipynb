{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "8Zd9EhXTqerj",
            "metadata": {
                "id": "8Zd9EhXTqerj"
            },
            "source": [
                "# Quickstart\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RelevanceAI/RelevanceAI-readme-docs/blob/v0.33.1/docs/GETTING_STARTED/example-applications/_notebooks/RelevanceAI_ReadMe_Quickstart_Text_to_Image_Search.ipynb)\n",
                "\n",
                "\n",
                "\n",
                "[Try the image search live in Relevance AI Dashboard](https://cloud.relevance.ai/demo/search/image-to-text).\n",
                "\n",
                "<img src=\"https://github.com/RelevanceAI/RelevanceAI-readme-docs/blob/v0.33.1/docs_template/GETTING_STARTED/example-applications/_assets/RelevanceAI_text_to_image.gif?raw=true\" \n",
                "     alt=\"RelevanceAI Text to Image\"\n",
                "     style=\"width: 100% vertical-align: middle\"/> \n",
                "\n",
                "\n",
                "In this notebook we will show you how to create and experiment with a powerful text to image search engine using OpenAI's CLIP and Relevance AI."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "928fd38d",
            "metadata": {
                "id": "928fd38d"
            },
            "source": [
                "# What I Need"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "31dfada7",
            "metadata": {
                "id": "31dfada7"
            },
            "source": [
                "- Project & API Key (The SDK will link you to the corresponding page or you can grab your API key from https://cloud.relevance.ai/ in the settings area)\n",
                "- Python 3\n",
                "- Relevance AI Installed as shown below. For more information visit [Installation guide](https://docs.relevance.ai/docs)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "VC8aPkNVQagh",
            "metadata": {
                "id": "VC8aPkNVQagh"
            },
            "source": [
                "## Installation Requirements"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "OOJCjWZkQb15",
            "metadata": {
                "id": "OOJCjWZkQb15"
            },
            "outputs": [],
            "source": [
                "# Relevance AI installation\n",
                "!pip install -U RelevanceAI[notebook]==0.33.1",
                "\n",
                "# Clip installation\n",
                "!pip install ftfy regex tqdm\n",
                "!pip install git+https://github.com/openai/CLIP.git"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dMV_xgPVVjtZ",
            "metadata": {
                "id": "dMV_xgPVVjtZ"
            },
            "source": [
                "## Client Setup\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "W9ndN9YSBDZC",
            "metadata": {
                "id": "W9ndN9YSBDZC"
            },
            "outputs": [],
            "source": [
                "# If True, you will get a shorter form!\n",
                "RUN_QUICK = True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "qsicetXLViu8",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "qsicetXLViu8",
                "outputId": "8d949e45-3365-4c09-8eb8-4156cd78f69d"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Authorization token (you can find it here: https://cloud.relevance.ai/sdk/api )\n",
                        "Auth token:\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"
                    ]
                }
            ],
            "source": [
                "from relevanceai import Client\n\n\"\"\"\nYou can sign up/login and find your credentials here: https://cloud.relevance.ai/sdk/api\nOnce you have signed up, click on the value under `Authorization token` and paste it here\n\"\"\"\nclient = Client()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "Si85Fxvtq4W5",
            "metadata": {
                "id": "Si85Fxvtq4W5"
            },
            "source": [
                "# Text-to-image search\n",
                "\n",
                "To enable text-to-image search we will be using Relevance AI as the vector database and OpenAI's CLIP as the vectorizer, to vectorize text and images into CLIP vector embeddings.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "keeAanisPnYx",
            "metadata": {
                "id": "keeAanisPnYx"
            },
            "source": [
                "## 1) Data\n",
                "For this quickstart we will be using a sample e-commerce dataset. Alternatively, you can use your own dataset for the different steps.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "I1NqdL82piq6",
            "metadata": {
                "id": "I1NqdL82piq6"
            },
            "outputs": [],
            "source": [
                "from relevanceai.datasets import get_ecommerce_2_dataset\n",
                "\n",
                "docs = get_ecommerce_2_dataset()\n",
                "{k:v for k, v in docs[0].items() if '_vector_' not in k}"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3ETgfqlEx00E",
            "metadata": {
                "id": "3ETgfqlEx00E"
            },
            "source": [
                "## 2) Encode - Vectorize with CLIP\n",
                "CLIP is a vectorizer from OpenAI that is trained to find similarities between text and image pairs. In the code below we set up CLIP. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0r5P1MwdPnYx",
            "metadata": {
                "id": "0r5P1MwdPnYx"
            },
            "outputs": [],
            "source": [
                "import torch\n",
                "import clip\n",
                "from PIL import Image\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "SVLADs3JPnYx",
            "metadata": {
                "id": "SVLADs3JPnYx"
            },
            "outputs": [],
            "source": [
                "import requests\n",
                "\n",
                "# Clip image encoding function\n",
                "def encode_image(image):\n",
                "    # Download the image and preprocess it\n",
                "    image = preprocess(Image.open(requests.get(image, stream=True).raw)).unsqueeze(0).to(device)\n",
                "    # Feed the processed image through the CLIP neural net to get the corresponding vector\n",
                "    with torch.no_grad():\n",
                "      image_features = model.encode_image(image)\n",
                "    # Lastly we convert it to a list so that we can send it through the SDK\n",
                "    return image_features.tolist()[0]\n",
                "\n",
                "# Clip text encoding function\n",
                "def encode_text(text):\n",
                "    # Get the text and tokenize it\n",
                "    text = clip.tokenize([text]).to(device)\n",
                "    # Feed the tokenized text through the CLIP neural net to get the corresponding vector\n",
                "    with torch.no_grad():\n",
                "        text_features = model.encode_text(text)\n",
                "    return text_features.tolist()[0]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eSGvSdtax8bO",
            "metadata": {
                "id": "eSGvSdtax8bO"
            },
            "source": [
                "We then encode the data we have into vectors, this will take a couple of mins"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "Z80cq1Wf3TSP",
            "metadata": {
                "id": "Z80cq1Wf3TSP"
            },
            "outputs": [],
            "source": [
                "if RUN_QUICK:\n",
                "  docs = docs[:500]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2dmZNIU0x5L6",
            "metadata": {
                "id": "2dmZNIU0x5L6"
            },
            "outputs": [],
            "source": [
                "from tqdm.notebook import tqdm\n",
                "\n",
                "def encode_image_document(d):\n",
                "    d['clip_product_image_vector_'] = encode_image(d['product_image'])\n",
                "\n",
                "encode_status = [encode_image_document(d) for d in tqdm(docs)]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "PfEdja6Hp3uy",
            "metadata": {
                "id": "PfEdja6Hp3uy"
            },
            "source": [
                "## 3) Insert"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "r-2qgvOLyPTi",
            "metadata": {
                "id": "r-2qgvOLyPTi"
            },
            "source": [
                "Uploading our documents into the dataset `quickstart_clip`.\n",
                "\n",
                "In case you are uploading your own dataset, keep in mind that each document should have a field called '_id'. Such an id can be easily allocated using the uuid package:\n",
                "\n",
                "```\n",
                "import uuid\n",
                "\n",
                "for d in docs:\n",
                "  d['_id'] = uuid.uuid4().__str__()    # Each document must have a field '_id'\n",
                "```\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ZAVHLeiNp55k",
            "metadata": {
                "id": "ZAVHLeiNp55k"
            },
            "outputs": [],
            "source": [
                "client.insert_documents(\"quickstart_clip\", docs)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "TLUEkCW_THXF",
            "metadata": {
                "id": "TLUEkCW_THXF"
            },
            "source": [
                "Once we have uploaded the data, we can see the dataset on the [dashboard](https://cloud.relevance.ai/dataset/quickstart_clip/dashboard/monitor/vectors). \n",
                "\n",
                "The dashboard provides users with a great overview and statistics of the dataset as shown below. "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "vHaP58MnTDX-",
            "metadata": {
                "id": "vHaP58MnTDX-"
            },
            "source": [
                "<img src=\"https://github.com/RelevanceAI/RelevanceAI-readme-docs/blob/v0.33.1/docs_template/GETTING_STARTED/example-applications/_assets/RelevanceAI_quickstart_clip_dashboard.png?raw=true\" width=\"1200\" alt=\"RelevanceAI Dashboard\" />"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "oLJKZwd1p8Wf",
            "metadata": {
                "id": "oLJKZwd1p8Wf"
            },
            "source": [
                "## 4) Search\n",
                "This step is to run a simple vector search; you can read more about vector search and how to construct a multi-vector query [here](https://docs.relevance.ai/docs/hybrid-search). \n",
                "\n",
                "Note that our dataset includes vectors generated by the Clip encoder. Therefore, in this step, we first vectorize the query using the same encoder to be able to search among the similarly generated vectors."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ahFxCSbGPnYy",
            "metadata": {
                "id": "ahFxCSbGPnYy"
            },
            "outputs": [],
            "source": [
                "query = \"for my baby daughter\"\n",
                "query_vector = encode_text(query) # vectorizing query\n",
                "\n",
                "results = client.services.search.vector(\n",
                "    dataset_id=\"quickstart_clip\",\n",
                "    multivector_query=[\n",
                "        {\n",
                "            \"vector\": query_vector,\n",
                "            \"fields\": [\"clip_product_image_vector_\"]\n",
                "        }\n",
                "    ],\n",
                "    page_size=5,\n",
                "    query=query\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "yQ1zWs5WdWuS",
            "metadata": {
                "id": "yQ1zWs5WdWuS"
            },
            "source": [
                "You can use our json shower library to observe the search result in a notebook as shown below:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "lmL7cE3vPnYy",
            "metadata": {
                "id": "lmL7cE3vPnYy"
            },
            "outputs": [],
            "source": [
                "from relevanceai import show_json\n",
                "\n",
                "print('=== QUERY ===', query)\n",
                "show_json(\n",
                "    results['results'],\n",
                "    image_fields=[\"product_image\"],\n",
                "    text_fields=[\"product_title\"]\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "reX6fXn4FfU_",
            "metadata": {
                "id": "reX6fXn4FfU_"
            },
            "source": [
                "# Projector \n",
                "\n",
                "Using Relevance AI projector, datapoints included in a datasets are projected into a 3D space"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "HReTYkDbEqE6",
            "metadata": {
                "id": "HReTYkDbEqE6"
            },
            "outputs": [],
            "source": [
                "client.projector.plot(\n",
                "    dataset_id=\"quickstart_clip\",\n",
                "    number_of_points_to_render=100,\n",
                "    vector_label = \"product_title\",\n",
                "    vector_field=\"clip_product_image_vector_\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "E_Lp28rM5X4G",
            "metadata": {
                "id": "E_Lp28rM5X4G"
            },
            "source": [
                "Other Notebooks:\n",
                "\n",
                "- [Multivector search with your own vectors](doc:search-with-your-own-vectors) \n",
                "- [Text search using USE (VectorHub)](doc:quickstart-text-search) \n",
                "- [Question answering using USE QA (Tensorflow Hub)](doc:quickstart-question-answering) "
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "collapsed_sections": [
                "8Zd9EhXTqerj"
            ],
            "name": "RelevanceAI-ReadMe-Text-to-Image-Search.ipynb",
            "provenance": [],
            "toc_visible": true
        },
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.10"
        },
        "toc": {
            "base_numbering": 1,
            "nav_menu": {},
            "number_sections": true,
            "sideBar": true,
            "skip_h1_title": false,
            "title_cell": "Table of Contents",
            "title_sidebar": "Contents",
            "toc_cell": false,
            "toc_position": {
                "height": "calc(100% - 180px)",
                "left": "10px",
                "top": "150px",
                "width": "384px"
            },
            "toc_section_display": true,
            "toc_window_display": true
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}